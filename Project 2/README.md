Project 2 - Web Crawler
In this project we were introduced to the structure of a web crawler as well as learning the different aspects of the HTTP protocol. Many resources helped us understand the concepts in this project that worked with the network layer.

The high-level approach we used a web crawler is to create a program that can send HTTP requests to a web server, receive and parse the response, extract CSRF token, session id, and possible links from the HTML body and headers, then repeat this process for multiple pages. The script should also handle different HTTP status codes, such as 302 Login was successful, 200 OK, 301 Moved Permanently, 403 Forbidden, 404 Not Found, and 500 Internal Server Error, and take appropriate actions based on the status code. We began by reading through the "HTTP Made Really Easy" to understand the structure of the HTTP messages and how we can communicate with the fakebook server. First we initialized a wrapped socket connection to connect to the HTTP host, and sent an initial GET request to the project2.5700.network server checking to see that we are returned a successful 200 Status code after which we could parse the header to receive the CSRF token and session ID. We then send another GET request and parse the response for a 200 Status code and extract any new cookies. We then use the newest set of cookies to send a POST request to the login page with the users username and password as the payload in the header. After this we check that we successfully logged in by checking for a 302 status code and again store any new cookies (CSRF and session ID). Now we can begin crawling the fakebook website using GET requests. We begin the web crawl utilizing the HTMLparser superclass and created our own HTMLparser subclass that searches for the start tags. The parser parses through the response to obtain the "a" tag that holds the URL and keeps searching for the next URLs storing them in a set for easy O(1) checking. If it hits a "h2" tag we have it split and store the secret flag in another set and move onto the next unvisited page. After a page has been parsed, it is considered visited and we add those to a set so it doesn't need to be checked again and a deque for the pages to visit which the far most link will get popped off and visited then added to the visitedPages set so it isn't parsed again. The crawling will continue throughout the URLs until 5 secret flags are found throughout the web pages upon which the script breaks. While the web crawler is searching through the pages, we also do error checking where if a status code is returned such as 301, 403, 404, or 500, the web crawler will undergo some manipulation based on what the error code is and what response is required of that error. Some of the challenges we faced in developing the web crawler script include handling the HTTP status codes, extracting the links and information from the HTML body, avoiding infinite loops or too many requests to the same page, and dealing with errors or exceptions.

At times testing was difficult as we ran into issues with receiving messages that didnâ€™t always split the header and body of the message, we had our program print out the message that would cause issues with our function that received the messages from the server and then manually tested those messages against the receive function. We also had to test the web crawler quite rigorously to avoid getting stuck in loops on one single page, we were able to determine that issue came from some error checking that we had improperly implemented in the subsequent steps. We initially had used the time module to keep track of how long the program would take to run through the fakebook website and we noticed that the time the program took was often over 5 minutes, we were able to reduce the look up time of the webpages visited by implementing a sets which have a constant look up time of O(1) this reduced the amount of time the program took to run to under 3 minutes for most situations.

For testing primarily, we had to add a lot of print statements at each phase we reached from the logging in portion all the way to the web crawling and if pages were actually getting added to the correct lists or not. We also tested the code with different inputs, such as different URLs, cookies, or headers, and verify that the output is correct and consistent.

We had some issues with figuring out how to work on the same codebase together as working on different functions of the web crawler created challenges with merging code. We were able to use GitHub and create separate branches to be able to separate functions and work separately on them and at the end merge our branches back in to the main.

Veniamin: worked on parts of the socket connections and helped with web crawling testing Sahil: worked on parts of the web crawler and error handling and testing.

